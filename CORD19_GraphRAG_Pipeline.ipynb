{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CORD-19 GraphRAG Pipeline\n",
        "\n",
        "A comprehensive pipeline for building knowledge graphs from the CORD-19 dataset, enabling semantic search and citation-aware summarization.\n",
        "\n",
        "## Overview\n",
        "This notebook implements a complete GraphRAG (Graph Retrieval-Augmented Generation) pipeline with the following steps:\n",
        "\n",
        "1. **Setup & Dataset Loading** - Initialize models and load CORD-19 data\n",
        "2. **Load & Preprocess Metadata** - Clean and prepare the dataset\n",
        "3. **Entity Extraction** - Extract biomedical entities using spaCy\n",
        "4. **Graph Construction** - Build knowledge graph with NetworkX\n",
        "5. **Semantic Embeddings** - Generate embeddings using SciBERT\n",
        "6. **FAISS Index** - Create vector index for fast retrieval\n",
        "7. **GraphRAG Retrieval** - Combine semantic search with graph context\n",
        "8. **GPT-4 Summarization** - Generate summaries with citations\n",
        "\n",
        "## Requirements\n",
        "```bash\n",
        "pip install pandas scispacy spacy transformers faiss-cpu networkx openai torch matplotlib seaborn tqdm\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup & Dataset Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"‚úÖ Loaded spaCy model: en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"‚ùå spaCy model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    print(\"   You can also try: pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SciBERT model for biomedical text\n",
        "print(\"üì• Loading SciBERT model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "print(\"‚úÖ Loaded SciBERT model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize OpenAI client (optional - set your API key)\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "if OPENAI_API_KEY:\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"‚úÖ OpenAI client initialized\")\n",
        "else:\n",
        "    client = None\n",
        "    print(\"‚ö†Ô∏è OpenAI API key not found. GPT-4 summarization will be skipped.\")\n",
        "    print(\"   Set your API key: export OPENAI_API_KEY='your-key-here'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load & Preprocess Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CORD-19 metadata\n",
        "metadata_path = \"2020-04-10/metadata.csv\"\n",
        "print(f\"üìÇ Loading metadata from {metadata_path}...\")\n",
        "\n",
        "df = pd.read_csv(metadata_path)\n",
        "print(f\"   Total papers: {len(df):,}\")\n",
        "\n",
        "# Keep only papers with abstracts\n",
        "df = df.dropna(subset=[\"abstract\"])\n",
        "print(f\"   Papers with abstracts: {len(df):,}\")\n",
        "\n",
        "# Select relevant columns\n",
        "df = df[[\"cord_uid\", \"title\", \"abstract\", \"authors\", \"journal\", \"publish_time\"]]\n",
        "\n",
        "print(\"\\nüìã Sample data:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset overview\n",
        "print(\"üìä Dataset Overview:\")\n",
        "print(f\"   Total papers: {len(df):,}\")\n",
        "print(f\"   Columns: {len(df.columns)}\")\n",
        "print(f\"   Date range: {df['publish_time'].min()} to {df['publish_time'].max()}\")\n",
        "\n",
        "# Analyze missing data\n",
        "print(\"\\nüìà Data Completeness:\")\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "\n",
        "for col in df.columns:\n",
        "    if missing_percent[col] > 0:\n",
        "        print(f\"   {col}: {missing_percent[col]:.1f}% missing ({missing_data[col]:,} papers)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sources and journals\n",
        "print(\"üìö Source Distribution:\")\n",
        "if 'source_x' in df.columns:\n",
        "    source_counts = df['source_x'].value_counts()\n",
        "    for source, count in source_counts.head(5).items():\n",
        "        print(f\"   {source}: {count:,} papers ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nüìñ Top 10 Journals:\")\n",
        "journal_counts = df['journal'].value_counts().head(10)\n",
        "for journal, count in journal_counts.items():\n",
        "    print(f\"   {journal}: {count:,} papers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Entity Extraction (SpaCy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define entity extraction function\n",
        "def extract_entities(text):\n",
        "    \"\"\"Extract entities from text using spaCy\"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Example on one abstract\n",
        "if len(df) > 0:\n",
        "    print(\"üîç Example entity extraction:\")\n",
        "    sample_text = df['abstract'].iloc[0]\n",
        "    entities = extract_entities(sample_text)\n",
        "    \n",
        "    print(f\"   Text: {sample_text[:200]}...\")\n",
        "    print(f\"   Entities found: {entities[:10]}\")  # Show first 10 entities\n",
        "    \n",
        "    # Show entity types\n",
        "    entity_types = [ent[1] for ent in entities]\n",
        "    entity_type_counts = pd.Series(entity_types).value_counts()\n",
        "    print(f\"\\n   Entity types: {dict(entity_type_counts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze entity distribution across a sample\n",
        "sample_size = 100\n",
        "sample_df = df.head(sample_size)\n",
        "\n",
        "print(f\"üß¨ Analyzing entities in {sample_size} papers...\")\n",
        "\n",
        "all_entities = []\n",
        "entity_types = []\n",
        "\n",
        "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Extracting entities\"):\n",
        "    entities = extract_entities(row['abstract'])\n",
        "    all_entities.extend([ent[0] for ent in entities])\n",
        "    entity_types.extend([ent[1] for ent in entities])\n",
        "\n",
        "# Analyze entity types\n",
        "entity_type_counts = pd.Series(entity_types).value_counts()\n",
        "print(f\"\\nüìä Entity Type Distribution:\")\n",
        "for entity_type, count in entity_type_counts.head(10).items():\n",
        "    print(f\"   {entity_type}: {count:,} entities\")\n",
        "\n",
        "# Plot entity types\n",
        "plt.figure(figsize=(12, 6))\n",
        "entity_type_counts.head(10).plot(kind='bar')\n",
        "plt.title('Top 10 Entity Types in CORD-19 Abstracts')\n",
        "plt.xlabel('Entity Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Graph Construction (NetworkX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Build graph from sample of papers\n",
        "sample_size = 500  # Limit for speed\n",
        "sample_df = df.head(sample_size)\n",
        "\n",
        "print(f\"üï∏Ô∏è Building graph from {len(sample_df)} papers...\")\n",
        "\n",
        "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Building graph\"):\n",
        "    paper_id = row['cord_uid']\n",
        "    \n",
        "    # Add paper node\n",
        "    G.add_node(paper_id, \n",
        "              type=\"paper\", \n",
        "              title=row[\"title\"],\n",
        "              journal=row[\"journal\"],\n",
        "              year=row[\"publish_time\"])\n",
        "    \n",
        "    # Add entities\n",
        "    entities = extract_entities(row[\"abstract\"])\n",
        "    for ent, label in entities:\n",
        "        # Clean entity text\n",
        "        ent_clean = ent.strip()\n",
        "        if len(ent_clean) > 1:  # Skip single characters\n",
        "            G.add_node(ent_clean, type=label)\n",
        "            G.add_edge(paper_id, ent_clean, relation=\"mentions\")\n",
        "    \n",
        "    # Add authors\n",
        "    if pd.notna(row[\"authors\"]):\n",
        "        authors = [author.strip() for author in row[\"authors\"].split(\";\") if author.strip()]\n",
        "        for author in authors:\n",
        "            G.add_node(author, type=\"author\")\n",
        "            G.add_edge(paper_id, author, relation=\"authored_by\")\n",
        "\n",
        "print(f\"‚úÖ Graph constructed:\")\n",
        "print(f\"   Nodes: {len(G.nodes):,}\")\n",
        "print(f\"   Edges: {len(G.edges):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze graph structure\n",
        "node_types = {}\n",
        "for node, data in G.nodes(data=True):\n",
        "    node_type = data.get('type', 'unknown')\n",
        "    node_types[node_type] = node_types.get(node_type, 0) + 1\n",
        "\n",
        "print(f\"üìä Node types:\")\n",
        "for node_type, count in sorted(node_types.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"   {node_type}: {count:,}\")\n",
        "\n",
        "# Plot node types\n",
        "plt.figure(figsize=(12, 6))\n",
        "node_type_counts = pd.Series(node_types)\n",
        "node_type_counts.plot(kind='bar')\n",
        "plt.title('Node Types in Knowledge Graph')\n",
        "plt.xlabel('Node Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Semantic Embeddings with SciBERT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define embedding function\n",
        "def embed(text):\n",
        "    \"\"\"Generate embeddings using SciBERT\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "# Test embedding on a sample text\n",
        "sample_text = \"COVID-19 is a respiratory disease caused by SARS-CoV-2 virus.\"\n",
        "sample_embedding = embed(sample_text)\n",
        "print(f\"‚úÖ Sample embedding shape: {sample_embedding.shape}\")\n",
        "print(f\"   Embedding dimension: {sample_embedding.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute embeddings for abstracts\n",
        "embedding_size = 1000  # Number of abstracts to embed\n",
        "abstracts = df[\"abstract\"].head(embedding_size).tolist()\n",
        "\n",
        "print(f\"üìä Generating embeddings for {len(abstracts)} abstracts...\")\n",
        "\n",
        "embeddings_list = []\n",
        "for i, abstract in enumerate(tqdm(abstracts, desc=\"Generating embeddings\")):\n",
        "    try:\n",
        "        emb = embed(abstract)\n",
        "        embeddings_list.append(emb)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Error processing abstract {i}: {e}\")\n",
        "        # Add zero embedding as fallback\n",
        "        embeddings_list.append(np.zeros((1, 768)))\n",
        "\n",
        "embeddings = np.vstack(embeddings_list)\n",
        "print(f\"‚úÖ Generated embeddings: {embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: FAISS Index for Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "print(f\"üìä Building FAISS index with dimension {dim}\")\n",
        "\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings.astype('float32'))\n",
        "\n",
        "print(f\"‚úÖ FAISS index built with {index.ntotal} vectors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test semantic search\n",
        "query = \"What drugs are being tested for COVID-19 treatment?\"\n",
        "\n",
        "def embed_query(query_text):\n",
        "    \"\"\"Embed query text\"\"\"\n",
        "    inputs = tokenizer(query_text, return_tensors=\"pt\", truncation=True, max_length=256, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy().astype('float32')\n",
        "\n",
        "q_vec = embed_query(query)\n",
        "D, I = index.search(q_vec, k=5)\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(f\"üìã Top 5 results:\")\n",
        "\n",
        "results_df = df.iloc[I[0]][[\"title\", \"abstract\"]]\n",
        "for i, (idx, row) in enumerate(results_df.iterrows()):\n",
        "    print(f\"\\n   {i+1}. {row['title'][:100]}...\")\n",
        "    print(f\"      {row['abstract'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: GraphRAG Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get graph context for retrieved papers\n",
        "def get_context_from_graph(paper_ids, G):\n",
        "    \"\"\"Get graph context for retrieved papers\"\"\"\n",
        "    context = []\n",
        "    for pid in paper_ids:\n",
        "        neighbors = list(G.neighbors(pid))\n",
        "        paper_data = G.nodes[pid]\n",
        "        title = paper_data.get('title', 'Unknown')\n",
        "        \n",
        "        context.append(f\"Paper '{title}' (ID: {pid}) mentions: {neighbors[:10]}\")  # Limit to first 10 neighbors\n",
        "    return \"\\n\".join(context)\n",
        "\n",
        "# Get context for our search results\n",
        "paper_ids = df.iloc[I[0]][\"cord_uid\"].tolist()\n",
        "graph_context = get_context_from_graph(paper_ids, G)\n",
        "\n",
        "print(\"üìä Graph context retrieved:\")\n",
        "print(graph_context[:1000] + \"...\" if len(graph_context) > 1000 else graph_context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: GPT-4 Summarization (OpenAI API)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary with GPT-4 (if API key is available)\n",
        "if client:\n",
        "    # Get abstracts for retrieved papers\n",
        "    context_text = \"\\n\\n\".join(df.iloc[I[0]][\"abstract\"].tolist())\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "You are a biomedical research assistant specializing in COVID-19 research.\n",
        "Summarize the following abstracts and their graph context in relation to the query.\n",
        "Cite specific entities, papers, and authors where relevant.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Abstracts:\n",
        "{context_text}\n",
        "\n",
        "Graph Context:\n",
        "{graph_context}\n",
        "\n",
        "Please provide a comprehensive summary with specific citations.\n",
        "\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        \n",
        "        summary = response.choices[0].message.content\n",
        "        print(\"‚úÖ Summary generated:\")\n",
        "        print(summary)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating summary: {e}\")\n",
        "        summary = None\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è OpenAI client not available. Skipping GPT-4 summarization.\")\n",
        "    print(\"\\nüìã Manual Summary of Retrieved Papers:\")\n",
        "    \n",
        "    # Manual summary of results\n",
        "    for i, (idx, row) in enumerate(results_df.iterrows()):\n",
        "        print(f\"\\n{i+1}. {row['title']}\")\n",
        "        print(f\"   Abstract: {row['abstract'][:300]}...\")\n",
        "    \n",
        "    summary = \"Manual summary provided above\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results directory\n",
        "Path(\"results\").mkdir(exist_ok=True)\n",
        "\n",
        "# Save graph\n",
        "nx.write_gml(G, \"results/cord19_graph.gml\")\n",
        "print(\"‚úÖ Graph saved to results/cord19_graph.gml\")\n",
        "\n",
        "# Save embeddings\n",
        "np.save(\"results/embeddings.npy\", embeddings)\n",
        "print(\"‚úÖ Embeddings saved to results/embeddings.npy\")\n",
        "\n",
        "# Save FAISS index\n",
        "faiss.write_index(index, \"results/faiss_index.bin\")\n",
        "print(\"‚úÖ FAISS index saved to results/faiss_index.bin\")\n",
        "\n",
        "# Save query results\n",
        "results = {\n",
        "    \"query\": query,\n",
        "    \"paper_ids\": paper_ids,\n",
        "    \"graph_context\": graph_context,\n",
        "    \"summary\": summary if 'summary' in locals() else \"No summary generated\",\n",
        "    \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(\"results/query_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(\"‚úÖ Query results saved to results/query_results.json\")\n",
        "\n",
        "# Save processed metadata\n",
        "df.to_csv(\"results/processed_metadata.csv\", index=False)\n",
        "print(\"‚úÖ Processed metadata saved to results/processed_metadata.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Query Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive query function\n",
        "def query_cord19(query_text, top_k=5):\n",
        "    \"\"\"Query the CORD-19 GraphRAG system\"\"\"\n",
        "    print(f\"üîç Querying: '{query_text}'\")\n",
        "    \n",
        "    # Generate query embedding\n",
        "    q_vec = embed_query(query_text)\n",
        "    \n",
        "    # Search FAISS index\n",
        "    D, I = index.search(q_vec, k=top_k)\n",
        "    \n",
        "    # Get results\n",
        "    results_df = df.iloc[I[0]][[\"title\", \"abstract\", \"authors\", \"journal\"]]\n",
        "    \n",
        "    print(f\"\\nüìã Top {top_k} results:\")\n",
        "    for i, (idx, row) in enumerate(results_df.iterrows()):\n",
        "        print(f\"\\n{i+1}. {row['title']}\")\n",
        "        print(f\"   Authors: {row['authors'][:100]}...\")\n",
        "        print(f\"   Journal: {row['journal']}\")\n",
        "        print(f\"   Abstract: {row['abstract'][:300]}...\")\n",
        "    \n",
        "    # Get graph context\n",
        "    paper_ids = df.iloc[I[0]][\"cord_uid\"].tolist()\n",
        "    graph_context = get_context_from_graph(paper_ids, G)\n",
        "    \n",
        "    return results_df, graph_context\n",
        "\n",
        "# Example queries\n",
        "example_queries = [\n",
        "    \"What are the symptoms of COVID-19?\",\n",
        "    \"How is SARS-CoV-2 transmitted?\",\n",
        "    \"What treatments are available for coronavirus?\",\n",
        "    \"What is the mortality rate of COVID-19?\",\n",
        "    \"How effective are masks in preventing COVID-19?\"\n",
        "]\n",
        "\n",
        "print(\"üí° Example queries you can try:\")\n",
        "for i, q in enumerate(example_queries, 1):\n",
        "    print(f\"   {i}. {q}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üéâ CORD-19 GraphRAG Pipeline Complete!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"‚úÖ Processed {len(df):,} papers\")\n",
        "print(f\"‚úÖ Built graph with {len(G.nodes):,} nodes and {len(G.edges):,} edges\")\n",
        "print(f\"‚úÖ Generated {embeddings.shape[0]:,} embeddings\")\n",
        "print(f\"‚úÖ Created FAISS index with {index.ntotal:,} vectors\")\n",
        "\n",
        "print(\"\\nüìÅ Results saved to 'results' directory:\")\n",
        "print(\"   - cord19_graph.gml: Knowledge graph\")\n",
        "print(\"   - embeddings.npy: SciBERT embeddings\")\n",
        "print(\"   - faiss_index.bin: Vector search index\")\n",
        "print(\"   - query_results.json: Query results\")\n",
        "print(\"   - processed_metadata.csv: Cleaned dataset\")\n",
        "\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"   1. Experiment with different queries\")\n",
        "print(\"   2. Increase sample sizes for better coverage\")\n",
        "print(\"   3. Add more sophisticated entity extraction\")\n",
        "print(\"   4. Implement graph-based reasoning\")\n",
        "print(\"   5. Add citation tracking and validation\")\n",
        "print(\"   6. Deploy as a web application\")\n",
        "\n",
        "print(\"\\nüí° Tips:\")\n",
        "print(\"   - Use specific biomedical terms for better results\")\n",
        "print(\"   - Try different query formulations\")\n",
        "print(\"   - Explore the graph structure for insights\")\n",
        "print(\"   - Combine multiple queries for comprehensive analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
